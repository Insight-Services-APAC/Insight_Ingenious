{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Insight Services APAC - Generative AI Collateral \u00b6 \"Accelerating time-to-value for Generative AI Projects.\" Mission Statement \u00b6 Guiding Principles \u00b6 SAAS over PAAS Selection Criteria for Approach to GenAI Business Value Focus Four Tier Architecture for Production Ready AI Projects Front end (React?) / M365 integrations (teams?) Middle Tier (Python/.Net?) GenAI Orchestrator (Python) Data Tier (Back End) (Fabric) Four Tier Architecture for POCs Front end (React)? Middle Tier (Python - Fast API?) AI Orchestrator (Python) (SmartSpace.Ai Workflow SDK?) Data Tier (Back End) (Fabric) Landing Zone Deployments (Bicep/Terraform?) Must Have Features \u00b6 Multi-Agent Orchestrator for Generative AI projects Rapidly deployed Fabric based POC for a generative AI project - full SaaS stack Ability (Tools?) Long-term conversation history Analyics Usage Reporting via Power BI User Feedback Nice to Have Features \u00b6 Industry Segment based Workflow solutions for common use cases (eg. Education: Curriculum Planning, Teacher Agent) Technologies to consider \u00b6 No/Low Code Solutions (Copilot Studio) Multi-Agent Orchestration Frameworks https://www.crewai.com/ https://microsoft.github.io/autogen/ https://smartspace.ai/ Front End Frameworks POC AutoGen Studio?? Notebook based UI? React Angular Vue Getting Started \u00b6 Documentation Guide For users who are looking to contribute to the documentation Documentation Guide","title":"Home"},{"location":"#insight-services-apac-generative-ai-collateral","text":"\"Accelerating time-to-value for Generative AI Projects.\"","title":"Insight Services APAC - Generative AI Collateral"},{"location":"#mission-statement","text":"","title":"Mission Statement"},{"location":"#guiding-principles","text":"SAAS over PAAS Selection Criteria for Approach to GenAI Business Value Focus Four Tier Architecture for Production Ready AI Projects Front end (React?) / M365 integrations (teams?) Middle Tier (Python/.Net?) GenAI Orchestrator (Python) Data Tier (Back End) (Fabric) Four Tier Architecture for POCs Front end (React)? Middle Tier (Python - Fast API?) AI Orchestrator (Python) (SmartSpace.Ai Workflow SDK?) Data Tier (Back End) (Fabric) Landing Zone Deployments (Bicep/Terraform?)","title":"Guiding Principles"},{"location":"#must-have-features","text":"Multi-Agent Orchestrator for Generative AI projects Rapidly deployed Fabric based POC for a generative AI project - full SaaS stack Ability (Tools?) Long-term conversation history Analyics Usage Reporting via Power BI User Feedback","title":"Must Have Features"},{"location":"#nice-to-have-features","text":"Industry Segment based Workflow solutions for common use cases (eg. Education: Curriculum Planning, Teacher Agent)","title":"Nice to Have Features"},{"location":"#technologies-to-consider","text":"No/Low Code Solutions (Copilot Studio) Multi-Agent Orchestration Frameworks https://www.crewai.com/ https://microsoft.github.io/autogen/ https://smartspace.ai/ Front End Frameworks POC AutoGen Studio?? Notebook based UI? React Angular Vue","title":"Technologies to consider"},{"location":"#getting-started","text":"Documentation Guide For users who are looking to contribute to the documentation Documentation Guide","title":"Getting Started"},{"location":"developer_guide/","text":"Developer Guide \u00b6","title":"Developer Guide"},{"location":"developer_guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"developer_guide/applications_setup/","text":"Applications Required for Development (Windows) \u00b6 This section covers the applications required for developing using the dbt framework. The applications are: python (latest version available) Visual Studio Code GIT for Windows OneLake Explorer (Preview) Install python \u00b6 First you need to install python. This can be done from this link Download Python | Python.org download the latest version and install it. NOTE: Make sure to tick the box on the first window of the installation to Add Python.exe to PATH Use Install now for standard installation. Once installed, confirm installation by opening the command line. WindowsKey + R then enter \u201ccmd\u201d click OK. Type the following and you should get a reply similar to the screenshot below. command prompt \u201cpython --version\u201d Install Visual Studio Code \u00b6 Browse to website Visual Studio Code - Code Editing. Redefined and download Visual Studio Code and then open the file. Select the standard options and install. Install GIT for Windows \u00b6 Browse to website Git - Downloading Package (git-scm.com) and download the latest by clicking \u201cClick here to download\u201d option. Open the file and install following the step examples. Select the standard options until you get these next steps. This step will ask you about your default git application please change this to Use Visual Studio Code as Git default. This step will ask you about your default git console, please select Use Windows default console window. The rest of the installation options should be standard unless you need to change them for other reasons. OneLake Explorer (Preview) \u00b6 Browse to website OneLake Explorer and browse down the page to Installation instructions and download OneLake file explorer and then open the file. Click install and follow standard install options. Opening up for the first time will require you to login using your Fabric tenant details. You will then be able to access the Lakehouses from your Windows Explorer. This concludes the required applications.","title":"Applications Required for Development (Windows)"},{"location":"developer_guide/applications_setup/#applications-required-for-development-windows","text":"This section covers the applications required for developing using the dbt framework. The applications are: python (latest version available) Visual Studio Code GIT for Windows OneLake Explorer (Preview)","title":"Applications Required for Development (Windows)"},{"location":"developer_guide/applications_setup/#install-python","text":"First you need to install python. This can be done from this link Download Python | Python.org download the latest version and install it. NOTE: Make sure to tick the box on the first window of the installation to Add Python.exe to PATH Use Install now for standard installation. Once installed, confirm installation by opening the command line. WindowsKey + R then enter \u201ccmd\u201d click OK. Type the following and you should get a reply similar to the screenshot below. command prompt \u201cpython --version\u201d","title":"Install python"},{"location":"developer_guide/applications_setup/#install-visual-studio-code","text":"Browse to website Visual Studio Code - Code Editing. Redefined and download Visual Studio Code and then open the file. Select the standard options and install.","title":"Install Visual Studio Code"},{"location":"developer_guide/applications_setup/#install-git-for-windows","text":"Browse to website Git - Downloading Package (git-scm.com) and download the latest by clicking \u201cClick here to download\u201d option. Open the file and install following the step examples. Select the standard options until you get these next steps. This step will ask you about your default git application please change this to Use Visual Studio Code as Git default. This step will ask you about your default git console, please select Use Windows default console window. The rest of the installation options should be standard unless you need to change them for other reasons.","title":"Install GIT for Windows"},{"location":"developer_guide/applications_setup/#onelake-explorer-preview","text":"Browse to website OneLake Explorer and browse down the page to Installation instructions and download OneLake file explorer and then open the file. Click install and follow standard install options. Opening up for the first time will require you to login using your Fabric tenant details. You will then be able to access the Lakehouses from your Windows Explorer. This concludes the required applications.","title":"OneLake Explorer (Preview)"},{"location":"developer_guide/dbt_setup/","text":"Setting Up dbt \u00b6 The following sections are covered in this document: Repo clone python environment setup dbt installation Repo Clone \u00b6 First you need to clone this repo locally using Visual studio code. For these instructions the feature/dev or dev branch will be the branches to work with. If you do not get the Clone Repository option when selecting Source Control from the menu, then you have not installed GIT and will need to complete that first. Python Environment Setup \u00b6 Once the repo has been cloned you can open a terminal window in VS Code and open a bash console. In new terminal window there is a plus symbol with a drop down. Select this drop down and click Git Bash. This will open a bash console. To create a virtual python environment execute the code below, \"dbt-env\" being the name of your virtual environment: # Python Virtual Environment python -m venv dbt-env To activate the environment execute this: # Python Virtual Environment source dbt-env/Scripts/activate The virtual environment would have created a folder structure in your repo. This can be excluded in your gitignore file. If you used the default above it is already in the gitignore file. dbt Installation \u00b6 Still in the bash console and having your virtual environment active, you can execute the following command to install all the components required for this dbt framework. The requirements.txt file is in the root of the repo. # dbt installation pip install -r requirements.txt NOTE: The installation can take sometime to complete. It may look like it's hanging but it is busy executing. If you close the installation you can restart it using the same command above. It will skip any components already installed. This concludes the dbt installation.","title":"Setting Up dbt"},{"location":"developer_guide/dbt_setup/#setting-up-dbt","text":"The following sections are covered in this document: Repo clone python environment setup dbt installation","title":"Setting Up dbt"},{"location":"developer_guide/dbt_setup/#repo-clone","text":"First you need to clone this repo locally using Visual studio code. For these instructions the feature/dev or dev branch will be the branches to work with. If you do not get the Clone Repository option when selecting Source Control from the menu, then you have not installed GIT and will need to complete that first.","title":"Repo Clone"},{"location":"developer_guide/dbt_setup/#python-environment-setup","text":"Once the repo has been cloned you can open a terminal window in VS Code and open a bash console. In new terminal window there is a plus symbol with a drop down. Select this drop down and click Git Bash. This will open a bash console. To create a virtual python environment execute the code below, \"dbt-env\" being the name of your virtual environment: # Python Virtual Environment python -m venv dbt-env To activate the environment execute this: # Python Virtual Environment source dbt-env/Scripts/activate The virtual environment would have created a folder structure in your repo. This can be excluded in your gitignore file. If you used the default above it is already in the gitignore file.","title":"Python Environment Setup"},{"location":"developer_guide/dbt_setup/#dbt-installation","text":"Still in the bash console and having your virtual environment active, you can execute the following command to install all the components required for this dbt framework. The requirements.txt file is in the root of the repo. # dbt installation pip install -r requirements.txt NOTE: The installation can take sometime to complete. It may look like it's hanging but it is busy executing. If you close the installation you can restart it using the same command above. It will skip any components already installed. This concludes the dbt installation.","title":"dbt Installation"},{"location":"developer_guide/folder_structure/","text":"Repo Folder Structure \u00b6 . \u251c\u2500\u2500 code_assets \u2502 \u251c\u2500\u2500 1_frontend_layer \u2502 \u251c\u2500\u2500 2_api_presentation_layer \u2502 \u251c\u2500\u2500 3_api_ai_orchestration_layer \u2502 \u251c\u2500\u2500 4a_ai_solution_layer \u2502 \u251c\u2500\u2500 4b_api_custom_tools \u2502 \u251c\u2500\u2500 5_data_ingestion \u2502 \u251c\u2500\u2500 6_data_prep \u2502 \u2514\u2500\u2500 7_data_serve \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u251c\u2500\u2500 diagrams \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u251c\u2500\u2500 developer_guide \u2502 \u2502 \u2514\u2500\u2500 images \u2502 \u251c\u2500\u2500 diagrams \u2502 \u2502 \u251c\u2500\u2500 drawio \u2502 \u2502 \u2502 \u2514\u2500\u2500 exports \u2502 \u2502 \u2514\u2500\u2500 plantuml \u2502 \u251c\u2500\u2500 documentation_guide \u2502 \u251c\u2500\u2500 javascripts \u2502 \u251c\u2500\u2500 overrides \u2502 \u251c\u2500\u2500 user_guide \u2502 \u2514\u2500\u2500 zzz_archive For documentation please go to https://effective-adventure-2kplkoq.pages.github.io/","title":"Repo Folder Structure"},{"location":"developer_guide/folder_structure/#repo-folder-structure","text":". \u251c\u2500\u2500 code_assets \u2502 \u251c\u2500\u2500 1_frontend_layer \u2502 \u251c\u2500\u2500 2_api_presentation_layer \u2502 \u251c\u2500\u2500 3_api_ai_orchestration_layer \u2502 \u251c\u2500\u2500 4a_ai_solution_layer \u2502 \u251c\u2500\u2500 4b_api_custom_tools \u2502 \u251c\u2500\u2500 5_data_ingestion \u2502 \u251c\u2500\u2500 6_data_prep \u2502 \u2514\u2500\u2500 7_data_serve \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 assets \u2502 \u2502 \u251c\u2500\u2500 diagrams \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2514\u2500\u2500 stylesheets \u2502 \u251c\u2500\u2500 developer_guide \u2502 \u2502 \u2514\u2500\u2500 images \u2502 \u251c\u2500\u2500 diagrams \u2502 \u2502 \u251c\u2500\u2500 drawio \u2502 \u2502 \u2502 \u2514\u2500\u2500 exports \u2502 \u2502 \u2514\u2500\u2500 plantuml \u2502 \u251c\u2500\u2500 documentation_guide \u2502 \u251c\u2500\u2500 javascripts \u2502 \u251c\u2500\u2500 overrides \u2502 \u251c\u2500\u2500 user_guide \u2502 \u2514\u2500\u2500 zzz_archive For documentation please go to https://effective-adventure-2kplkoq.pages.github.io/","title":"Repo Folder Structure"},{"location":"developer_guide/framework_setup/","text":"Framework Setup \u00b6 Fabric Workspace Setup \u00b6 Here we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below: Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace. Ensure that the workspace is Fabric enabled. If not, enable it. Make sure that there is at least one Datalake in the workspace. Get the connection details for the workspace. Get the lakehouse name, the workspace id, and the lakehouse id. The lakehouse name and workspace name are easily viewed from the fabric / power bi portal. The easiest way to get this information is to Navigate to a file or folder in the lakehouse, click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window. From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL. In the example below, the workspace id is 4f0cb887-047a-48a1-98c3-ebdb38c784c2 and the lakehouse id is aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9 . https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks Create Dbt Project \u00b6 Once you have taken note of the workspace id, lakehouse id, and lakehouse name, you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below: !> Important Note when asked to select the adapter choose dbt-fabricksparknb . If you can't see the adapter, first install the dbt-fabricsparknb package from repository. During this process you will also be asked for the workspace id , lakehouse id , and lakehouse name . Use the values you gathered from the Power BI Portal. # Create your dbt project directories and profiles.yml file dbt init my_project # Note that the name of the project is arbitrary... call it whatever you like The command above will create a new directory called my_project . Within this directory you will find a dbt_project.yml file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.: # Name your project! Project names should contain only lowercase characters # and underscores. A good package name should reflect your organization's # name or the intended use of these models name : 'my_project' version : '1.0.0' # This setting configures which \"profile\" dbt uses for this project. profile : 'my_project' # These configurations specify where dbt should look for different types of files. # The `model-paths` config, for example, states that models in this project can be # found in the \"models/\" directory. You probably won't need to change these! model-paths : [ \"models\" ] analysis-paths : [ \"analyses\" ] test-paths : [ \"tests\" ] seed-paths : [ \"seeds\" ] macro-paths : [ \"macros\" ] snapshot-paths : [ \"snapshots\" ] clean-targets : # directories to be removed by `dbt clean` - \"target\" - \"dbt_packages\" # Configuring models # Full documentation: https://docs.getdbt.com/docs/configuring-models # In this example config, we tell dbt to build all models in the example/ # directory as views. These settings can be overridden in the individual model # files using the ` config(...) ` macro. models : test4 : # Config indicated by + and applies to all files under models/example/ example : +materialized : view The dbt init command will also update your profiles.yml file with a profile matching your dbt project name. Open this file in VS Code. This file can be found in \"./%USER_DIRECTORY%/.dbt/\" When run this will display a file similar to the one below. Check that your details are correct. my_project : outputs : dev : auth : cli #remove client_id : dlkdjl #remove client_scrent : dlkdjl #remove connect_retries : 0 #remove connect_timeout : 0 #remove endpoint : dkld #remove lakehouse : 'lakehouse' #the name of your lakehouse lakehouseid : 'aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9' #the guid of your lakehouse method : livy schema : dbo #the schema you want to use tenant_id : '72f988bf-86f1-41af-91ab-2d7cd011db47' #your power bi tenant id threads : 1 #the number of threads to use type : fabricsparknb #the type of adapter to use.. always use fabricsparknb workspaceid : '4f0cb887-047a-48a1-98c3-ebdb38c784c2' #the guid of your workspace target : dev To complete the newly created project you will need to copy some directories from the project called \"ptfproj\" dbt project. Copy ptfproj/macros/ and ptfproj/metaextracts/ directories with their files into your new dbt project. Overwrite any directories or files if they exist. Now in metaextracts the file ListSchemas.json contains the lakehouses in your workspace. You can manually update this file. [{ \"namespace\" : \"lh_raw\" },{ \"namespace\" : \"lh_conformed\" },{ \"namespace\" : \"lh_consolidated\" }] Create a build python Script \u00b6 This repo contains a dbt build python script test_pre_install.py in the root. from dbt.adapters.fabricsparknb import utils as utils import os import sys utils . RunDbtProjectArg ( PreInstall = True , argv = sys . argv ) You can execute this file by passing your project name as the parameter python test_pre_install.py my_project If you get an error with Azure CLI connection issues or type errors. This is because the Profile.yaml file has the incorrect adaptor set. It should be \"fabricsparknb\" not \"fabricspark\" . After successful execution and number of notebooks have been created in your project/target folder under notebooks. import_notebook.ipynb this will be used to import notebook files into your lakehouse. metadata_extract.ipynb is used to update the metadata json files in your project. The above two notebooks can be imported using the standard import notebooks function in fabric. The rest of the notebooks can be copied into your lakehouse Files/notebooks folder by running the following script in pwsh. #Run upload.ps1 Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / upload . ps1 You then open the import_notebook.ipynb in fabric and Run All to import the notebooks from the Files/Notebooks directory in fabric. Similar to upload, using the following pwsh script will help you to download the metaextract files to the metaextrcats folder in repo. #Run upload.ps1 Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / download . ps1 Executing the master_notebook.ipynb notebook will execute all notebooks created in your project. This concludes the Framework setup.","title":"Framework Setup"},{"location":"developer_guide/framework_setup/#framework-setup","text":"","title":"Framework Setup"},{"location":"developer_guide/framework_setup/#fabric-workspace-setup","text":"Here we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below: Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace. Ensure that the workspace is Fabric enabled. If not, enable it. Make sure that there is at least one Datalake in the workspace. Get the connection details for the workspace. Get the lakehouse name, the workspace id, and the lakehouse id. The lakehouse name and workspace name are easily viewed from the fabric / power bi portal. The easiest way to get this information is to Navigate to a file or folder in the lakehouse, click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window. From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL. In the example below, the workspace id is 4f0cb887-047a-48a1-98c3-ebdb38c784c2 and the lakehouse id is aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9 . https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks","title":"Fabric Workspace Setup"},{"location":"developer_guide/framework_setup/#create-dbt-project","text":"Once you have taken note of the workspace id, lakehouse id, and lakehouse name, you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below: !> Important Note when asked to select the adapter choose dbt-fabricksparknb . If you can't see the adapter, first install the dbt-fabricsparknb package from repository. During this process you will also be asked for the workspace id , lakehouse id , and lakehouse name . Use the values you gathered from the Power BI Portal. # Create your dbt project directories and profiles.yml file dbt init my_project # Note that the name of the project is arbitrary... call it whatever you like The command above will create a new directory called my_project . Within this directory you will find a dbt_project.yml file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.: # Name your project! Project names should contain only lowercase characters # and underscores. A good package name should reflect your organization's # name or the intended use of these models name : 'my_project' version : '1.0.0' # This setting configures which \"profile\" dbt uses for this project. profile : 'my_project' # These configurations specify where dbt should look for different types of files. # The `model-paths` config, for example, states that models in this project can be # found in the \"models/\" directory. You probably won't need to change these! model-paths : [ \"models\" ] analysis-paths : [ \"analyses\" ] test-paths : [ \"tests\" ] seed-paths : [ \"seeds\" ] macro-paths : [ \"macros\" ] snapshot-paths : [ \"snapshots\" ] clean-targets : # directories to be removed by `dbt clean` - \"target\" - \"dbt_packages\" # Configuring models # Full documentation: https://docs.getdbt.com/docs/configuring-models # In this example config, we tell dbt to build all models in the example/ # directory as views. These settings can be overridden in the individual model # files using the ` config(...) ` macro. models : test4 : # Config indicated by + and applies to all files under models/example/ example : +materialized : view The dbt init command will also update your profiles.yml file with a profile matching your dbt project name. Open this file in VS Code. This file can be found in \"./%USER_DIRECTORY%/.dbt/\" When run this will display a file similar to the one below. Check that your details are correct. my_project : outputs : dev : auth : cli #remove client_id : dlkdjl #remove client_scrent : dlkdjl #remove connect_retries : 0 #remove connect_timeout : 0 #remove endpoint : dkld #remove lakehouse : 'lakehouse' #the name of your lakehouse lakehouseid : 'aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9' #the guid of your lakehouse method : livy schema : dbo #the schema you want to use tenant_id : '72f988bf-86f1-41af-91ab-2d7cd011db47' #your power bi tenant id threads : 1 #the number of threads to use type : fabricsparknb #the type of adapter to use.. always use fabricsparknb workspaceid : '4f0cb887-047a-48a1-98c3-ebdb38c784c2' #the guid of your workspace target : dev To complete the newly created project you will need to copy some directories from the project called \"ptfproj\" dbt project. Copy ptfproj/macros/ and ptfproj/metaextracts/ directories with their files into your new dbt project. Overwrite any directories or files if they exist. Now in metaextracts the file ListSchemas.json contains the lakehouses in your workspace. You can manually update this file. [{ \"namespace\" : \"lh_raw\" },{ \"namespace\" : \"lh_conformed\" },{ \"namespace\" : \"lh_consolidated\" }]","title":"Create Dbt Project"},{"location":"developer_guide/framework_setup/#create-a-build-python-script","text":"This repo contains a dbt build python script test_pre_install.py in the root. from dbt.adapters.fabricsparknb import utils as utils import os import sys utils . RunDbtProjectArg ( PreInstall = True , argv = sys . argv ) You can execute this file by passing your project name as the parameter python test_pre_install.py my_project If you get an error with Azure CLI connection issues or type errors. This is because the Profile.yaml file has the incorrect adaptor set. It should be \"fabricsparknb\" not \"fabricspark\" . After successful execution and number of notebooks have been created in your project/target folder under notebooks. import_notebook.ipynb this will be used to import notebook files into your lakehouse. metadata_extract.ipynb is used to update the metadata json files in your project. The above two notebooks can be imported using the standard import notebooks function in fabric. The rest of the notebooks can be copied into your lakehouse Files/notebooks folder by running the following script in pwsh. #Run upload.ps1 Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / upload . ps1 You then open the import_notebook.ipynb in fabric and Run All to import the notebooks from the Files/Notebooks directory in fabric. Similar to upload, using the following pwsh script will help you to download the metaextract files to the metaextrcats folder in repo. #Run upload.ps1 Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / download . ps1 Executing the master_notebook.ipynb notebook will execute all notebooks created in your project. This concludes the Framework setup.","title":"Create a build python Script"},{"location":"developer_guide/initial_setup/","text":"\ud83d\udce6dbt \u2523 \ud83d\udcc2adapters \u2503 \u2523 \ud83d\udcc2fabricsparknb \u2503 \u2503 \u2523 \ud83d\udcdc init .py \u2503 \u2503 \u2523 \ud83d\udcdc version .py \u2503 \u2503 \u2523 \ud83d\udcdccatalog.py \u2503 \u2503 \u2523 \ud83d\udcdcconnections.py \u2503 \u2503 \u2523 \ud83d\udcdcfabric_spark_credentials.py \u2503 \u2503 \u2523 \ud83d\udcdcimpl.py \u2503 \u2503 \u2523 \ud83d\udcdclivysession.py \u2503 \u2503 \u2523 \ud83d\udcdcmanifest.py \u2503 \u2503 \u2523 \ud83d\udcdcmock.py \u2503 \u2503 \u2523 \ud83d\udcdcnotebook.py \u2503 \u2503 \u2517 \ud83d\udcdcutils.py \u2503 \u2517 \ud83d\udcdc init .py \u2523 \ud83d\udcc2include \u2503 \u2523 \ud83d\udcc2fabricsparknb \u2503 \u2503 \u2523 \ud83d\udcc2macros \u2503 \u2503 \u2503 \u2517 \ud83d\udcc2adapters \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcschema.sql \u2503 \u2503 \u2523 \ud83d\udcc2notebooks \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcimport_notebook.ipynb \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmaster_notebook.ipynb \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmaster_notebook_x.ipynb \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmetadata_extract.ipynb \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmodel_notebook.ipynb \u2503 \u2503 \u2503 \u2517 \ud83d\udcdctest_notebook.ipynb \u2503 \u2503 \u2523 \ud83d\udcc2pwsh \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcdownload.ps1 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcupload.ps1 \u2503 \u2503 \u2523 \ud83d\udcdc init .py \u2503 \u2503 \u2523 \ud83d\udcdcdbt_project.yml \u2503 \u2503 \u2523 \ud83d\udcdcmetadata_extract.ipynb \u2503 \u2503 \u2517 \ud83d\udcdcprofile_template.yml \u2503 \u2517 \ud83d\udcdc init .py \u2517 \ud83d\udcdc init .py","title":"Initial setup"},{"location":"documentation_guide/","text":"Documentation Guide \u00b6 Building you environment \u00b6 Documentation for this project is built using mkdocs-material . To contribute to the documentation you will need to create a separate python environment. I suggest that you call this .env_mkdocs to avoid confusion with the dbt environment. Create your environment and install the required packages as shown below: Important The commands below assume that you have already performed the Core Tools Installation steps in the User Guide . If you have not done this yet, please do so before proceeding. Note you ONLY have to install core tools it is not necessary to move on to the other tools section. Create and activate the Python environment # Create and activate the Python environment python -m venv . env_mkdocs .\\. env_mkdocs \\ Scripts \\ activate . ps1 pip install -r ./ requirements_mkdocs . txt Updating the documentation \u00b6 The docucementation source is held in the docs directory. To update the documentation you will need to edit the markdown files in this directory. In order to understand the syntax used for the markdown be sure to review the reference section for mkdocs-material . Once you have made your changes you can build the documentation using the command below: Build the documentation mkdocs build To view the documentation locally you can use the command below: View the documentation locally mkdocs serve Tip The mkdocs serve command will start a local web server that will allow you to view the documentation in your browser. The server will also automatically rebuild the documentation when you make changes to the source files. Before publishing the documentation you should ensure that the documentation is up to date and that the changes are correct. You should also pull the latest from the repository to ensure that you are not overwriting someone else's changes. Do this by running the command below: Pull the latest changes from the repository git pull You can now publish the documentation to the repository by running the command below: Publish the documentation mkdocs gh-deploy","title":"Documentation Guide"},{"location":"documentation_guide/#documentation-guide","text":"","title":"Documentation Guide"},{"location":"documentation_guide/#building-you-environment","text":"Documentation for this project is built using mkdocs-material . To contribute to the documentation you will need to create a separate python environment. I suggest that you call this .env_mkdocs to avoid confusion with the dbt environment. Create your environment and install the required packages as shown below: Important The commands below assume that you have already performed the Core Tools Installation steps in the User Guide . If you have not done this yet, please do so before proceeding. Note you ONLY have to install core tools it is not necessary to move on to the other tools section. Create and activate the Python environment # Create and activate the Python environment python -m venv . env_mkdocs .\\. env_mkdocs \\ Scripts \\ activate . ps1 pip install -r ./ requirements_mkdocs . txt","title":"Building you environment"},{"location":"documentation_guide/#updating-the-documentation","text":"The docucementation source is held in the docs directory. To update the documentation you will need to edit the markdown files in this directory. In order to understand the syntax used for the markdown be sure to review the reference section for mkdocs-material . Once you have made your changes you can build the documentation using the command below: Build the documentation mkdocs build To view the documentation locally you can use the command below: View the documentation locally mkdocs serve Tip The mkdocs serve command will start a local web server that will allow you to view the documentation in your browser. The server will also automatically rebuild the documentation when you make changes to the source files. Before publishing the documentation you should ensure that the documentation is up to date and that the changes are correct. You should also pull the latest from the repository to ensure that you are not overwriting someone else's changes. Do this by running the command below: Pull the latest changes from the repository git pull You can now publish the documentation to the repository by running the command below: Publish the documentation mkdocs gh-deploy","title":"Updating the documentation"},{"location":"user_guide/","text":"User Guide \u00b6","title":"User Guide"},{"location":"user_guide/#user-guide","text":"","title":"User Guide"},{"location":"user_guide/dbt_build_process/","text":"Dbt Build Process \u00b6 Run the build script \u00b6 Run the build script that you created in the previous step using the code below in the terminal. Important Be sure to replace my_project with the name of your dbt project folder. python post_install . py my_project Additional Steps for the First Run \u00b6 The first time that you run this script you will get a warning that the metadata json files are not present. You will need to follow the steps in the warning message to download the metadata json files from your Fabric Lakehouse. These steps are as follows: In a pwsh terminal run the script below. This will set an environment variable that will be used by steps later on. Be sure to replace my_project with the name of your dbt project folder. $env:DBT_PROJECT_DIR = \"my_project\" In the same terminal now run the script below. This will upload a series of notebooks to a folder in your Fabric workspace Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / upload . ps1 Now run the following code to output the path to the import notebook. This will be used to import the notebooks into your workspace: Get-Childitem \"$env:DBT_PROJECT_DIR/target/notebooks/import_*\" | % { Write-Host $_ . FullName } Now login to the Fabric Portal and navigate to the workspace and lakehouse you are using and import the notebook using the path from the previous step. How to manually upload a notebook See https://learn.microsoft.com/en-us/fabric/data-engineering/how-to-use-notebook#import-existing-notebooks Open the notebook in the workspace and run all cells. This will upload the generated notebooks to your workspace. A new notebook should appear in the workspace called metadata_ my_project _extract.ipynb where the text my_project is replaced with the name of your dbt_project. Open this notebook and run all cells. This will generate the metadata extract json files and place them in the metaextracts sub-directory of your lakehouse. Now, back in your powershell terminal. Run the script below. This will download the metadata extract json files to a subfolder in your dbt project directory called metaextracts. Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / download . ps1 Important Note that the above script runs azcopy to download the metadata extract json files from the lakehouse. Prior to running the script you may need to log in to the appropriate environment. Usually this is done using azcopy login . However, if you have access to multiple Microsoft entra domains you may need to specify the tenant id. This can be done by running azcopy login --tenant-id <tenant_id> . Now re-run the dbt build script. python post_install . py my_project Important In the script above be sure to replace my_project with the name of your dbt project folder. When you run this build script successfully, you will see a series of notebooks generated in your my_project /target/notebooks directory. This is the \"special sauce\" of this dbt-adapter that allows your to run your dbt project natively as notebooks in a Fabric workspace. The image below shows a sample listing of generated notebooks. Your specific notebooks will be contain the name of your dbt project and may be different depending on the models and tests that you have defined in your dbt project. Sample listing of Generated Notebooks \u00b6 If you study the files shown above you will notice that there is a naming convention and that the notebooks are prefixed with a specific string. The following table explains at a high level the naming convention and the purpose of each type of notebook. Notebook Prefix Description model. These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. test. These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. seed. These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. master_ These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality import_ This is a helper notebook that facilitate import of generated notebooks into workspace. metadata_ This is a helper notebook to facilitate generation of workspace metadata json files. Important The green panels below provide a more detailed discussion of each type of notebook. Take a moment to expand each panel by clicking on it and read the detailed explanation of each type of notebook. Notebooks with the Prefix \"model.\" These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. Notebooks with the Prefix \"test.\" These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. Notebooks with the Prefix \"seed.\" These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. Notebooks with the Prefix \"master_\" These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality. Notebooks with the Prefix \"import_\" This is a helper notebook that facilitates import of generated notebooks into workspace. Notebooks with the Prefix \"metadata_\" This is a helper notebook to facilitates the generation of workspace metadata json files. Post Build Steps & Checks \u00b6 After a successful execution you can now upload your notebooks to your Fabric Workspace and run your dbt transformations against your lakehouse/s. To do this follow the steps below: In a pwsh terminal run the script below. This will set an environment variable that will be used by steps later on. Be sure to replace my_project with the name of your dbt project folder. $env:DBT_PROJECT_DIR = \"my_project\" In the same terminal now run the script below. This will upload a the generated notebooks to a folder in your Fabric workspace Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / upload . ps1 In your Fabric workspace open the import_ my_project _notebook and run all cells. This will upload the generated notebooks to your workspace. If they already exist they will be updated. In order to organise your workspace efficiently I suggest that you move them all into a folder called the matches the name of your dbt project. Don't worry, even if you upload a new version of the notebooks the folder structure will be mantained.","title":"Dbt Build Process"},{"location":"user_guide/dbt_build_process/#dbt-build-process","text":"","title":"Dbt Build Process"},{"location":"user_guide/dbt_build_process/#run-the-build-script","text":"Run the build script that you created in the previous step using the code below in the terminal. Important Be sure to replace my_project with the name of your dbt project folder. python post_install . py my_project","title":"Run the build script"},{"location":"user_guide/dbt_build_process/#additional-steps-for-the-first-run","text":"The first time that you run this script you will get a warning that the metadata json files are not present. You will need to follow the steps in the warning message to download the metadata json files from your Fabric Lakehouse. These steps are as follows: In a pwsh terminal run the script below. This will set an environment variable that will be used by steps later on. Be sure to replace my_project with the name of your dbt project folder. $env:DBT_PROJECT_DIR = \"my_project\" In the same terminal now run the script below. This will upload a series of notebooks to a folder in your Fabric workspace Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / upload . ps1 Now run the following code to output the path to the import notebook. This will be used to import the notebooks into your workspace: Get-Childitem \"$env:DBT_PROJECT_DIR/target/notebooks/import_*\" | % { Write-Host $_ . FullName } Now login to the Fabric Portal and navigate to the workspace and lakehouse you are using and import the notebook using the path from the previous step. How to manually upload a notebook See https://learn.microsoft.com/en-us/fabric/data-engineering/how-to-use-notebook#import-existing-notebooks Open the notebook in the workspace and run all cells. This will upload the generated notebooks to your workspace. A new notebook should appear in the workspace called metadata_ my_project _extract.ipynb where the text my_project is replaced with the name of your dbt_project. Open this notebook and run all cells. This will generate the metadata extract json files and place them in the metaextracts sub-directory of your lakehouse. Now, back in your powershell terminal. Run the script below. This will download the metadata extract json files to a subfolder in your dbt project directory called metaextracts. Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / download . ps1 Important Note that the above script runs azcopy to download the metadata extract json files from the lakehouse. Prior to running the script you may need to log in to the appropriate environment. Usually this is done using azcopy login . However, if you have access to multiple Microsoft entra domains you may need to specify the tenant id. This can be done by running azcopy login --tenant-id <tenant_id> . Now re-run the dbt build script. python post_install . py my_project Important In the script above be sure to replace my_project with the name of your dbt project folder. When you run this build script successfully, you will see a series of notebooks generated in your my_project /target/notebooks directory. This is the \"special sauce\" of this dbt-adapter that allows your to run your dbt project natively as notebooks in a Fabric workspace. The image below shows a sample listing of generated notebooks. Your specific notebooks will be contain the name of your dbt project and may be different depending on the models and tests that you have defined in your dbt project.","title":"Additional Steps for the First Run"},{"location":"user_guide/dbt_build_process/#sample-listing-of-generated-notebooks","text":"If you study the files shown above you will notice that there is a naming convention and that the notebooks are prefixed with a specific string. The following table explains at a high level the naming convention and the purpose of each type of notebook. Notebook Prefix Description model. These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. test. These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. seed. These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. master_ These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality import_ This is a helper notebook that facilitate import of generated notebooks into workspace. metadata_ This is a helper notebook to facilitate generation of workspace metadata json files. Important The green panels below provide a more detailed discussion of each type of notebook. Take a moment to expand each panel by clicking on it and read the detailed explanation of each type of notebook. Notebooks with the Prefix \"model.\" These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. Notebooks with the Prefix \"test.\" These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. Notebooks with the Prefix \"seed.\" These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. Notebooks with the Prefix \"master_\" These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality. Notebooks with the Prefix \"import_\" This is a helper notebook that facilitates import of generated notebooks into workspace. Notebooks with the Prefix \"metadata_\" This is a helper notebook to facilitates the generation of workspace metadata json files.","title":"Sample listing of Generated Notebooks"},{"location":"user_guide/dbt_build_process/#post-build-steps-checks","text":"After a successful execution you can now upload your notebooks to your Fabric Workspace and run your dbt transformations against your lakehouse/s. To do this follow the steps below: In a pwsh terminal run the script below. This will set an environment variable that will be used by steps later on. Be sure to replace my_project with the name of your dbt project folder. $env:DBT_PROJECT_DIR = \"my_project\" In the same terminal now run the script below. This will upload a the generated notebooks to a folder in your Fabric workspace Invoke-Expression -Command $env:DBT_PROJECT_DIR / target / pwsh / upload . ps1 In your Fabric workspace open the import_ my_project _notebook and run all cells. This will upload the generated notebooks to your workspace. If they already exist they will be updated. In order to organise your workspace efficiently I suggest that you move them all into a folder called the matches the name of your dbt project. Don't worry, even if you upload a new version of the notebooks the folder structure will be mantained.","title":"Post Build Steps &amp; Checks"},{"location":"user_guide/dbt_project_setup/","text":"DBT Project Setup \u00b6 Fabric Workspace Setup \u00b6 Next we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below: Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace. Ensure that the workspace is Fabric enabled. If not, enable it. Make sure that there is at least one Datalake in the workspace. Get the connection details for the workspace. You will need to get the workspace name , workspace id , lakehouse id , and lakehouse name . The lakehouse name and workspace name are easily viewed from the fabric / power bi portal. The easiest way to get the id information is to: Navigate to a file or folder in your target lakehouse. Click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window. From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL. In the example below, the workspace id is 4f0cb887-047a-48a1-98c3-ebdb38c784c2 and the lakehouse id is aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9 . Example URL https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks Create Dbt Project \u00b6 Once you have taken note of the workspace id , lakehouse id , workspace name and lakehouse name you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below: # Create your dbt project directories and profiles.yml file dbt init my_project # Note that the name of the project is arbitrary... call it whatever you like When asked the questions below, provide the answers in bold below: Which data base would you like to use? select dbt-fabricksparknb Desired authentication method option (enter a number): select livy workspaceid (GUID of the workspace. Open the workspace from fabric.microsoft.com and copy the workspace url): Enter the workspace id lakehouse (Name of the Lakehouse in the workspace that you want to connect to): Enter the lakehouse name lakehouseid (GUID of the lakehouse, which can be extracted from url when you open lakehouse artifact from fabric.microsoft.com): Enter the lakehouse id endpoint [https://api.fabric.microsoft.com/v1]: Press enter to accept the default auth (Use CLI (az login) for interactive execution or SPN for automation) [CLI]: select cli client_id (Use when SPN auth is used.): Enter a single space and press enter client_scrent (Use when SPN auth is used.): Enter a single space and press enter tenant_id (Use when SPN auth is used.): Enter a single space or Enter your PowerBI tenant id connect_retries [0]: Enter 0 connect_timeout [10]: Enter 10 schema (default schema that dbt will build objects in): Enter dbo threads (1 or more) [1]: Enter 1 The command above will create a new directory called my_project . Within this directory you will find a dbt_project.yml file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.: dbt_project.yml # Name your project! Project names should contain only lowercase characters # and underscores. A good package name should reflect your organization's # name or the intended use of these models name : 'my_project' version : '1.0.0' # This setting configures which \"profile\" dbt uses for this project. profile : 'my_project' # These configurations specify where dbt should look for different types of files. # The `model-paths` config, for example, states that models in this project can be # found in the \"models/\" directory. You probably won't need to change these! model-paths : [ \"models\" ] analysis-paths : [ \"analyses\" ] test-paths : [ \"tests\" ] seed-paths : [ \"seeds\" ] macro-paths : [ \"macros\" ] snapshot-paths : [ \"snapshots\" ] clean-targets : # directories to be removed by `dbt clean` - \"target\" - \"dbt_packages\" # Configuring models # Full documentation: https://docs.getdbt.com/docs/configuring-models models : test4 : # Config indicated by + and applies to all files under models/example/ example : +materialized : view The dbt init command will also update your profiles.yml file with a profile matching your dbt project name. Open this file in your favourite text editor using the command below: Windows MacOS Linux code $home /. dbt / profiles . yml code ~/. dbt / profiles . yml code ~/. dbt / profiles . yml When run this will display a file similar to the one below. Check that your details are correct. Note The profiles.yml file should look like the example below except that in your case the highlighted lines may contain different values. profiles.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 my_project : target : my_project_target outputs : my_project_target : authentication : CLI method : livy connect_retries : 0 connect_timeout : 10 endpoint : https://api.fabric.microsoft.com/v1 workspaceid : 4f0cb887-047a-48a1-98c3-ebdb38c784c2 workspacename : test lakehousedatapath : /lakehouse lakehouseid : 031feff6-071d-42df-818a-984771c083c4 lakehouse : datalake schema : dbo threads : 1 type : fabricsparknb retry_all : true Now we are ready to run our dbt project for the first time. But first we need to create a build script. Create a python build script that will wrap our dbt process \u00b6 This repository contains a dbt build script created in python. Make a copy of this script by copying the code found at https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb/blob/main/test_post_install.py . Alternatively, you can copy the code in the code block titled Python Build script template below. Paste the code into a new file in the root of your source code directory. You can create this file using the vscode command line shown in the code block titled New file creation in vscode below. Important Be sure to change the line os.environ['DBT_PROJECT_DIR'] = \"testproj\" by replacing \"testproj\" with the folder name of your dbt project. New file creation in vscode code post_install . py Python Build script template from dbt.adapters.fabricsparknb import utils as utils import os import sys utils . RunDbtProjectArg ( PreInstall = False , argv = sys . argv ) Info You are now ready to move to the next step in which you will build your dbt project. Follow the Dbt Build Process guide.","title":"DBT Project Setup"},{"location":"user_guide/dbt_project_setup/#dbt-project-setup","text":"","title":"DBT Project Setup"},{"location":"user_guide/dbt_project_setup/#fabric-workspace-setup","text":"Next we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below: Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace. Ensure that the workspace is Fabric enabled. If not, enable it. Make sure that there is at least one Datalake in the workspace. Get the connection details for the workspace. You will need to get the workspace name , workspace id , lakehouse id , and lakehouse name . The lakehouse name and workspace name are easily viewed from the fabric / power bi portal. The easiest way to get the id information is to: Navigate to a file or folder in your target lakehouse. Click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window. From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL. In the example below, the workspace id is 4f0cb887-047a-48a1-98c3-ebdb38c784c2 and the lakehouse id is aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9 . Example URL https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks","title":"Fabric Workspace Setup"},{"location":"user_guide/dbt_project_setup/#create-dbt-project","text":"Once you have taken note of the workspace id , lakehouse id , workspace name and lakehouse name you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below: # Create your dbt project directories and profiles.yml file dbt init my_project # Note that the name of the project is arbitrary... call it whatever you like When asked the questions below, provide the answers in bold below: Which data base would you like to use? select dbt-fabricksparknb Desired authentication method option (enter a number): select livy workspaceid (GUID of the workspace. Open the workspace from fabric.microsoft.com and copy the workspace url): Enter the workspace id lakehouse (Name of the Lakehouse in the workspace that you want to connect to): Enter the lakehouse name lakehouseid (GUID of the lakehouse, which can be extracted from url when you open lakehouse artifact from fabric.microsoft.com): Enter the lakehouse id endpoint [https://api.fabric.microsoft.com/v1]: Press enter to accept the default auth (Use CLI (az login) for interactive execution or SPN for automation) [CLI]: select cli client_id (Use when SPN auth is used.): Enter a single space and press enter client_scrent (Use when SPN auth is used.): Enter a single space and press enter tenant_id (Use when SPN auth is used.): Enter a single space or Enter your PowerBI tenant id connect_retries [0]: Enter 0 connect_timeout [10]: Enter 10 schema (default schema that dbt will build objects in): Enter dbo threads (1 or more) [1]: Enter 1 The command above will create a new directory called my_project . Within this directory you will find a dbt_project.yml file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.: dbt_project.yml # Name your project! Project names should contain only lowercase characters # and underscores. A good package name should reflect your organization's # name or the intended use of these models name : 'my_project' version : '1.0.0' # This setting configures which \"profile\" dbt uses for this project. profile : 'my_project' # These configurations specify where dbt should look for different types of files. # The `model-paths` config, for example, states that models in this project can be # found in the \"models/\" directory. You probably won't need to change these! model-paths : [ \"models\" ] analysis-paths : [ \"analyses\" ] test-paths : [ \"tests\" ] seed-paths : [ \"seeds\" ] macro-paths : [ \"macros\" ] snapshot-paths : [ \"snapshots\" ] clean-targets : # directories to be removed by `dbt clean` - \"target\" - \"dbt_packages\" # Configuring models # Full documentation: https://docs.getdbt.com/docs/configuring-models models : test4 : # Config indicated by + and applies to all files under models/example/ example : +materialized : view The dbt init command will also update your profiles.yml file with a profile matching your dbt project name. Open this file in your favourite text editor using the command below: Windows MacOS Linux code $home /. dbt / profiles . yml code ~/. dbt / profiles . yml code ~/. dbt / profiles . yml When run this will display a file similar to the one below. Check that your details are correct. Note The profiles.yml file should look like the example below except that in your case the highlighted lines may contain different values. profiles.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 my_project : target : my_project_target outputs : my_project_target : authentication : CLI method : livy connect_retries : 0 connect_timeout : 10 endpoint : https://api.fabric.microsoft.com/v1 workspaceid : 4f0cb887-047a-48a1-98c3-ebdb38c784c2 workspacename : test lakehousedatapath : /lakehouse lakehouseid : 031feff6-071d-42df-818a-984771c083c4 lakehouse : datalake schema : dbo threads : 1 type : fabricsparknb retry_all : true Now we are ready to run our dbt project for the first time. But first we need to create a build script.","title":"Create Dbt Project"},{"location":"user_guide/dbt_project_setup/#create-a-python-build-script-that-will-wrap-our-dbt-process","text":"This repository contains a dbt build script created in python. Make a copy of this script by copying the code found at https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb/blob/main/test_post_install.py . Alternatively, you can copy the code in the code block titled Python Build script template below. Paste the code into a new file in the root of your source code directory. You can create this file using the vscode command line shown in the code block titled New file creation in vscode below. Important Be sure to change the line os.environ['DBT_PROJECT_DIR'] = \"testproj\" by replacing \"testproj\" with the folder name of your dbt project. New file creation in vscode code post_install . py Python Build script template from dbt.adapters.fabricsparknb import utils as utils import os import sys utils . RunDbtProjectArg ( PreInstall = False , argv = sys . argv ) Info You are now ready to move to the next step in which you will build your dbt project. Follow the Dbt Build Process guide.","title":"Create a python build script that will wrap our dbt process"},{"location":"user_guide/development_workflow/","text":"Development Workflow \u00b6 Development and Deployment Flow Using Fabric Spark Notebook Adapter \u00b6 Advantages Available today Native Fabric Notebooks Generated and Deployed Non dbt users able to view notebooks and business logic Monitoring and debugging of loads directly in Fabric without the need for a separate tool Re-occurring loads achieved using native Fabric scheduling Simplified code promotion process using native Fabric Git integration No need for dbt hosted in a virtual machine No need for service account No need for Azure Landing Zone No need for secure network connectivity between Azure VM and Fabric Allows for disconnected development environment providing Faster DBT build times Greater developer flexibility Simplified code promotion Process using native Fabric Git integration Single, native promotion process for all Fabric artifacts including non-dbt ones Disadvantages Requires Additional Steps Meta Data Extract Notebook Upload Notebook Import Development and Deployment Flow Using Original Fabric Spark Adapter \u00b6 Detailed Workflow \u00b6 Inital Setup 1. Provision Workspace - Development Environment: Fabric Portal - Re-occurence: Do once per development environment set-up - Instructions: Create a new workspace in the Power BI Portal, or use an existing workspace. Get Workspace Connection Details Development Environment: Fabric Portal Re-occurence: Do once per development environment set-up Instructions: Get the workspace connection details from the Power BI Portal. Create or Update profiles.yml Development Environment: VS Code on local, developemnt machine Create or Update dbt_project.yml Build Project Manually Upload Notebooks Run Meta Data Extract Ongoing Development Cycle Download Metadata: Update Dbt Project Build Dbt Project Verify Outputs Update Notebooks Upload to Onelake Update to GIT repo Promote to Workspace Run Import Notebook Promote GIT branch Run Master Notebook Validate Results Run Metadata Extract","title":"Development Workflow"},{"location":"user_guide/development_workflow/#development-workflow","text":"","title":"Development Workflow"},{"location":"user_guide/development_workflow/#development-and-deployment-flow-using-fabric-spark-notebook-adapter","text":"Advantages Available today Native Fabric Notebooks Generated and Deployed Non dbt users able to view notebooks and business logic Monitoring and debugging of loads directly in Fabric without the need for a separate tool Re-occurring loads achieved using native Fabric scheduling Simplified code promotion process using native Fabric Git integration No need for dbt hosted in a virtual machine No need for service account No need for Azure Landing Zone No need for secure network connectivity between Azure VM and Fabric Allows for disconnected development environment providing Faster DBT build times Greater developer flexibility Simplified code promotion Process using native Fabric Git integration Single, native promotion process for all Fabric artifacts including non-dbt ones Disadvantages Requires Additional Steps Meta Data Extract Notebook Upload Notebook Import","title":"Development and Deployment Flow Using Fabric Spark Notebook Adapter"},{"location":"user_guide/development_workflow/#development-and-deployment-flow-using-original-fabric-spark-adapter","text":"","title":"Development and Deployment Flow Using Original Fabric Spark Adapter"},{"location":"user_guide/development_workflow/#detailed-workflow","text":"Inital Setup 1. Provision Workspace - Development Environment: Fabric Portal - Re-occurence: Do once per development environment set-up - Instructions: Create a new workspace in the Power BI Portal, or use an existing workspace. Get Workspace Connection Details Development Environment: Fabric Portal Re-occurence: Do once per development environment set-up Instructions: Get the workspace connection details from the Power BI Portal. Create or Update profiles.yml Development Environment: VS Code on local, developemnt machine Create or Update dbt_project.yml Build Project Manually Upload Notebooks Run Meta Data Extract Ongoing Development Cycle Download Metadata: Update Dbt Project Build Dbt Project Verify Outputs Update Notebooks Upload to Onelake Update to GIT repo Promote to Workspace Run Import Notebook Promote GIT branch Run Master Notebook Validate Results Run Metadata Extract","title":"Detailed Workflow"},{"location":"user_guide/fabric_ci_cd_process/","text":"Fabric CI/CD with Git Deployment \u00b6 Git Based Deployment \u00b6 The initial setup is based on a Git branch that is linked to all workspaces. As illustrated in the given example, we have described three stages: Development, Test, and Production. It also employs feature branches for individual developments within isolated workspaces using branch out functionality. The successful operation of this scenario depends on branching, merging, and pull requests. Each workspace is assigned its own branch. The introduction of new features is facilitated by raising pull requests. All deployments are initiated from the repository. To transition from Development to Test, and subsequently from Test to Production, a pull request must be initiated from the originating stage. The synchronization between the Git branch and the workspace can be automated. This is achieved by invoking the Git Sync API as part of a build pipelines, which is automatically triggered following the approval of a pull request. Git and Build Environment Based Deployment \u00b6 Git is exclusively linked to the Development workspace. The deployment to other stages is executed based on Build environments. This implies that the Fabric Item APIs are utilized to perform Create, Read, Update or Delete operations. Key points of this setup are: The Git repository serves as the foundation for creating, updating, or deleting items in the workspace. Git is solely connected to the Development workspace. Following a pull request, a Build pipeline is activated. The Build pipeline executes operations to the workspace. Note This approach is code-intensive and for each future item to be supported, modifications may be required in the Build pipelines. Git and Fabric Deployment Pipeline Based Deployment \u00b6 This is based on Fabric Deployment pipelines. This user-friendly interface simplifies the deployment process from one stage to another and is less code-intensive. Git is solely connected to the Development workspace, and feature branches continue to exist in separate workspaces. However, the Test, Production, and any additional workspaces are not linked to Git. Key aspects of this setup include: The release process to other stages, such as Test and Production, is managed via Deployment Pipelines in the Fabric. The Development workspace is the only one connected to Git. Triggers for the Fabric deployment pipeline can be automated. This is achieved by using Build Pipelines, which are automatically activated following the approval of a pull request. These pipelines can call the Fabric REST API and can also be integrated with Git Sync API for synchronizing the development workspace.","title":"Fabric CI/CD with Git Deployment"},{"location":"user_guide/fabric_ci_cd_process/#fabric-cicd-with-git-deployment","text":"","title":"Fabric CI/CD with Git Deployment"},{"location":"user_guide/fabric_ci_cd_process/#git-based-deployment","text":"The initial setup is based on a Git branch that is linked to all workspaces. As illustrated in the given example, we have described three stages: Development, Test, and Production. It also employs feature branches for individual developments within isolated workspaces using branch out functionality. The successful operation of this scenario depends on branching, merging, and pull requests. Each workspace is assigned its own branch. The introduction of new features is facilitated by raising pull requests. All deployments are initiated from the repository. To transition from Development to Test, and subsequently from Test to Production, a pull request must be initiated from the originating stage. The synchronization between the Git branch and the workspace can be automated. This is achieved by invoking the Git Sync API as part of a build pipelines, which is automatically triggered following the approval of a pull request.","title":"Git Based Deployment"},{"location":"user_guide/fabric_ci_cd_process/#git-and-build-environment-based-deployment","text":"Git is exclusively linked to the Development workspace. The deployment to other stages is executed based on Build environments. This implies that the Fabric Item APIs are utilized to perform Create, Read, Update or Delete operations. Key points of this setup are: The Git repository serves as the foundation for creating, updating, or deleting items in the workspace. Git is solely connected to the Development workspace. Following a pull request, a Build pipeline is activated. The Build pipeline executes operations to the workspace. Note This approach is code-intensive and for each future item to be supported, modifications may be required in the Build pipelines.","title":"Git and Build Environment Based Deployment"},{"location":"user_guide/fabric_ci_cd_process/#git-and-fabric-deployment-pipeline-based-deployment","text":"This is based on Fabric Deployment pipelines. This user-friendly interface simplifies the deployment process from one stage to another and is less code-intensive. Git is solely connected to the Development workspace, and feature branches continue to exist in separate workspaces. However, the Test, Production, and any additional workspaces are not linked to Git. Key aspects of this setup include: The release process to other stages, such as Test and Production, is managed via Deployment Pipelines in the Fabric. The Development workspace is the only one connected to Git. Triggers for the Fabric deployment pipeline can be automated. This is achieved by using Build Pipelines, which are automatically activated following the approval of a pull request. These pipelines can call the Fabric REST API and can also be integrated with Git Sync API for synchronizing the development workspace.","title":"Git and Fabric Deployment Pipeline Based Deployment"},{"location":"user_guide/initial_setup/","text":"Environment Setup \u00b6 This section outlines the steps required to setup the development environment to use this dbt-adapter as part of a dbt data transformation project. To provide a common, cross-platform set of instructions we will first install Powershell. To facilitate the installation process we will use package managers such as winget for Windows, brew for MacOS and apt for Linux. Core Tools Installation \u00b6 Windows MacOS Linux # Winget Installs winget install Microsoft . PowerShell brew install powershell/tap/powershell # TBA Next we will install Python and development tools such as vscode. Windows MacOS Linux # Winget Installs winget install -e - -id Python . Python -v 3 . 12 winget install -e - -id Microsoft . VisualStudioCode winget install - -id Git . Git -e - -source winget # Python Environment Manager Python -m pip install - -user virtualenv # Brew Installs brew install python@3.12 brew install --cask visual-studio-code brew install git # Python Environment Manager Python -m pip install --user virtualenv # TODO # Add OSX AZ Copy Instructions # TBA Other tools \u00b6 Now that we have pwsh installed, we can use it as a cross platform shell to install the additional required tools. Windows MacOS Linux # Az Copy Install - No Winget Package Available Invoke-WebRequest -Uri https :// aka . ms / downloadazcopy-v10-windows -OutFile AzCopy . zip -UseBasicParsing Expand-Archive ./ AzCopy . zip ./ AzCopy -Force New-Item -ItemType \"directory\" -Path \"$home/AzCopy\" -Force Get-ChildItem ./ AzCopy /*/ azcopy . exe | Move-Item -Destination \"$home\\AzCopy\\AzCopy.exe\" -Force $userenv = [System.Environment] :: GetEnvironmentVariable ( \"Path\" , \"User\" ) [System.Environment] :: SetEnvironmentVariable ( \"PATH\" , $userenv + \";$home\\AzCopy\" , \"User\" ) Remove-Item .\\ AzCopy \\ -Force Remove-Item AzCopy . zip -Force # TODO # Add OSX AZ Copy Instructions # TBA Source Directory & Python Env \u00b6 Now lets create and activate our Python environment and install the required packages. Tip When doing pip install dbt-fabricspark below it can take a few minutes to complete on some machines. Occasionally pip may get stuck and in such cases break the execution using ctrl-c and run the same pip again. Windows MacOS Linux # Ensure that you are in the pwsh shell pwsh # Create a new source code directory mkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like # Navigate to the new directory cd dbt-fabricsparknb-test # Create and activate the Python environment python -m venv . env ./. env / Scripts / Activate . ps1 # Install the dbt-fabricsparknb package from the repository pip install - -upgrade git + https :// github . com / Insight-Services-APAC / APAC-Capability-DAI-DbtFabricSparkNb # Ensure that you are in the pwsh shell pwsh # Create a new source code directory mkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like # Navigate to the new directory cd dbt-fabricsparknb-test # Create and activate the Python environment python -m venv . env ./. env / Scripts / Activate . ps1 # Install the dbt-fabricsparknb package from the repository pip install - -upgrade git + https :// github . com / Insight-Services-APAC / APAC-Capability-DAI-DbtFabricSparkNb # TBA Info You are now ready to move to the next step in which you will set up your dbt project. Follow the Dbt Project Setup guide.","title":"Environment Setup"},{"location":"user_guide/initial_setup/#environment-setup","text":"This section outlines the steps required to setup the development environment to use this dbt-adapter as part of a dbt data transformation project. To provide a common, cross-platform set of instructions we will first install Powershell. To facilitate the installation process we will use package managers such as winget for Windows, brew for MacOS and apt for Linux.","title":"Environment Setup"},{"location":"user_guide/initial_setup/#core-tools-installation","text":"Windows MacOS Linux # Winget Installs winget install Microsoft . PowerShell brew install powershell/tap/powershell # TBA Next we will install Python and development tools such as vscode. Windows MacOS Linux # Winget Installs winget install -e - -id Python . Python -v 3 . 12 winget install -e - -id Microsoft . VisualStudioCode winget install - -id Git . Git -e - -source winget # Python Environment Manager Python -m pip install - -user virtualenv # Brew Installs brew install python@3.12 brew install --cask visual-studio-code brew install git # Python Environment Manager Python -m pip install --user virtualenv # TODO # Add OSX AZ Copy Instructions # TBA","title":"Core Tools Installation"},{"location":"user_guide/initial_setup/#other-tools","text":"Now that we have pwsh installed, we can use it as a cross platform shell to install the additional required tools. Windows MacOS Linux # Az Copy Install - No Winget Package Available Invoke-WebRequest -Uri https :// aka . ms / downloadazcopy-v10-windows -OutFile AzCopy . zip -UseBasicParsing Expand-Archive ./ AzCopy . zip ./ AzCopy -Force New-Item -ItemType \"directory\" -Path \"$home/AzCopy\" -Force Get-ChildItem ./ AzCopy /*/ azcopy . exe | Move-Item -Destination \"$home\\AzCopy\\AzCopy.exe\" -Force $userenv = [System.Environment] :: GetEnvironmentVariable ( \"Path\" , \"User\" ) [System.Environment] :: SetEnvironmentVariable ( \"PATH\" , $userenv + \";$home\\AzCopy\" , \"User\" ) Remove-Item .\\ AzCopy \\ -Force Remove-Item AzCopy . zip -Force # TODO # Add OSX AZ Copy Instructions # TBA","title":"Other tools"},{"location":"user_guide/initial_setup/#source-directory-python-env","text":"Now lets create and activate our Python environment and install the required packages. Tip When doing pip install dbt-fabricspark below it can take a few minutes to complete on some machines. Occasionally pip may get stuck and in such cases break the execution using ctrl-c and run the same pip again. Windows MacOS Linux # Ensure that you are in the pwsh shell pwsh # Create a new source code directory mkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like # Navigate to the new directory cd dbt-fabricsparknb-test # Create and activate the Python environment python -m venv . env ./. env / Scripts / Activate . ps1 # Install the dbt-fabricsparknb package from the repository pip install - -upgrade git + https :// github . com / Insight-Services-APAC / APAC-Capability-DAI-DbtFabricSparkNb # Ensure that you are in the pwsh shell pwsh # Create a new source code directory mkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like # Navigate to the new directory cd dbt-fabricsparknb-test # Create and activate the Python environment python -m venv . env ./. env / Scripts / Activate . ps1 # Install the dbt-fabricsparknb package from the repository pip install - -upgrade git + https :// github . com / Insight-Services-APAC / APAC-Capability-DAI-DbtFabricSparkNb # TBA Info You are now ready to move to the next step in which you will set up your dbt project. Follow the Dbt Project Setup guide.","title":"Source Directory &amp; Python Env"}]}