{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Insight Services APAC - Generative AI Collateral","text":"<p><p>\"Accelerating time-to-value for Generative AI Projects.\"</p></p>"},{"location":"#mission-statement","title":"Mission Statement","text":""},{"location":"#guiding-principles","title":"Guiding Principles","text":"<ol> <li>SAAS over PAAS</li> <li>Selection Criteria for Approach to GenAI</li> <li>Business Value Focus</li> <li>Four Tier Architecture for Production Ready AI Projects<ul> <li>Front end (React?) / M365 integrations (teams?)</li> <li>Middle Tier (Python/.Net?)</li> <li>GenAI Orchestrator (Python)</li> <li>Data Tier (Back End) (Fabric)</li> </ul> </li> <li>Four Tier Architecture for POCs<ul> <li>Front end (React)?</li> <li>Middle Tier (Python - Fast API?)</li> <li>AI Orchestrator (Python) (SmartSpace.Ai Workflow SDK?)</li> <li>Data Tier (Back End) (Fabric)</li> </ul> </li> <li>Landing Zone Deployments (Bicep/Terraform?)</li> </ol>"},{"location":"#must-have-features","title":"Must Have Features","text":"<ul> <li> Multi-Agent Orchestrator for Generative AI projects</li> <li> Rapidly deployed Fabric based POC for a generative AI project - full SaaS stack</li> <li> Ability (Tools?)</li> <li> Long-term conversation history</li> <li> Analyics Usage Reporting via Power BI</li> <li> User Feedback</li> </ul>"},{"location":"#nice-to-have-features","title":"Nice to Have Features","text":"<ul> <li> Industry Segment based Workflow solutions for common use cases (eg. Education: Curriculum Planning, Teacher Agent)</li> </ul>"},{"location":"#technologies-to-consider","title":"Technologies to consider","text":"<ul> <li>No/Low Code Solutions (Copilot Studio)</li> <li>Multi-Agent Orchestration Frameworks</li> <li>https://www.crewai.com/</li> <li>https://microsoft.github.io/autogen/</li> <li>https://smartspace.ai/</li> <li>Front End Frameworks</li> <li>POC <ul> <li>AutoGen Studio??  </li> <li>Notebook based UI?</li> </ul> </li> <li>React</li> <li>Angular</li> <li>Vue</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Documentation Guide</p> <p>For users who are looking to contribute to the documentation</p> <p> Documentation Guide</p> </li> </ul>"},{"location":"developer_guide/","title":"Developer Guide","text":""},{"location":"developer_guide/applications_setup/","title":"Applications Required for Development (Windows)","text":"<p>This section covers the applications required for developing using the dbt framework. The applications are:</p> <ul> <li>python (latest version available)</li> <li>Visual Studio Code</li> <li>GIT for Windows</li> <li>OneLake Explorer (Preview)</li> </ul>"},{"location":"developer_guide/applications_setup/#install-python","title":"Install python","text":"<p>First you need to install python. This can be done from this link Download Python | Python.org download the latest version and install it. </p> <p>NOTE: Make sure to tick the box on the first window of the installation to Add Python.exe to PATH</p> <p>Use Install now for standard installation.</p> <p>Once installed, confirm installation by opening the command line. WindowsKey + R then enter \u201ccmd\u201d click OK. </p> <p>Type the following and you should get a reply similar to the screenshot below. <code>command prompt  \u201cpython --version\u201d</code></p> <p></p>"},{"location":"developer_guide/applications_setup/#install-visual-studio-code","title":"Install Visual Studio Code","text":"<p>Browse to website Visual Studio Code - Code Editing. Redefined and download Visual Studio Code and then open the file.</p> <p>Select the standard options and install.</p>"},{"location":"developer_guide/applications_setup/#install-git-for-windows","title":"Install GIT for Windows","text":"<p>Browse to website Git - Downloading Package (git-scm.com) and download the latest by clicking \u201cClick here to download\u201d option. Open the file and install following the step examples.</p> <p></p> <p>Select the standard options until you get these next steps.</p> <p>This step will ask you about your default git application please change this to Use Visual Studio Code as Git default. </p> <p></p> <p>This step will ask you about your default git console, please select Use Windows default console window.</p> <p></p> <p>The rest of the installation options should be standard unless you need to change them for other reasons.</p>"},{"location":"developer_guide/applications_setup/#onelake-explorer-preview","title":"OneLake Explorer (Preview)","text":"<p>Browse to website OneLake Explorer and browse down the page to Installation instructions and download OneLake file explorer and then open the file.</p> <p>Click install and follow standard install options. </p> <p>Opening up for the first time will require you to login using your Fabric tenant details. You will then be able to access the Lakehouses from your Windows Explorer.</p> <p></p> <p>This concludes the required applications.</p>"},{"location":"developer_guide/dbt_setup/","title":"Setting Up dbt","text":"<p>The following sections are covered in this document:</p> <ul> <li>Repo clone</li> <li>python environment setup</li> <li>dbt installation</li> </ul>"},{"location":"developer_guide/dbt_setup/#repo-clone","title":"Repo Clone","text":"<p>First you need to clone this repo locally using Visual studio code. For these instructions the feature/dev or dev branch will be the branches to work with.</p> <p>If you do not get the Clone Repository option when selecting Source Control from the menu, then you have not installed GIT and will need to complete that first.</p> <p></p>"},{"location":"developer_guide/dbt_setup/#python-environment-setup","title":"Python Environment Setup","text":"<p>Once the repo has been cloned you can open a terminal window in VS Code and open a bash console.</p> <p>In new terminal window there is a plus symbol with a drop down. Select this drop down and click Git Bash. This will open a bash console. </p> <p></p> <p>To create a virtual python environment execute the code below, \"dbt-env\" being the name of your virtual environment: <pre><code># Python Virtual Environment\npython -m venv dbt-env\n</code></pre></p> <p>To activate the environment execute this: <pre><code># Python Virtual Environment\nsource dbt-env/Scripts/activate\n</code></pre></p> <p></p> <p>The virtual environment would have created a folder structure in your repo. This can be excluded in your gitignore file. If you used the default above it is already in the gitignore file.</p>"},{"location":"developer_guide/dbt_setup/#dbt-installation","title":"dbt Installation","text":"<p>Still in the bash console and having your virtual environment active, you can execute the following command to install all the components required for this dbt framework. The requirements.txt file is in the root of the repo. <pre><code># dbt installation\npip install -r requirements.txt\n</code></pre> NOTE: The installation can take sometime to complete. It may look like it's hanging but it is busy executing. If you close the installation you can restart it using the same command above. It will skip any components already installed. </p> <p>This concludes the dbt installation.</p>"},{"location":"developer_guide/folder_structure/","title":"Repo Folder Structure","text":"<p>. \u251c\u2500\u2500 code_assets \u2502   \u251c\u2500\u2500 1_frontend_layer \u2502   \u251c\u2500\u2500 2_api_presentation_layer \u2502   \u251c\u2500\u2500 3_api_ai_orchestration_layer \u2502   \u251c\u2500\u2500 4a_ai_solution_layer \u2502   \u251c\u2500\u2500 4b_api_custom_tools \u2502   \u251c\u2500\u2500 5_data_ingestion \u2502   \u251c\u2500\u2500 6_data_prep \u2502   \u2514\u2500\u2500 7_data_serve \u251c\u2500\u2500 docs \u2502   \u251c\u2500\u2500 assets \u2502   \u2502   \u251c\u2500\u2500 diagrams \u2502   \u2502   \u251c\u2500\u2500 images \u2502   \u2502   \u2514\u2500\u2500 stylesheets \u2502   \u251c\u2500\u2500 developer_guide \u2502   \u2502   \u2514\u2500\u2500 images \u2502   \u251c\u2500\u2500 diagrams \u2502   \u2502   \u251c\u2500\u2500 drawio \u2502   \u2502   \u2502   \u2514\u2500\u2500 exports \u2502   \u2502   \u2514\u2500\u2500 plantuml \u2502   \u251c\u2500\u2500 documentation_guide \u2502   \u251c\u2500\u2500 javascripts \u2502   \u251c\u2500\u2500 overrides \u2502   \u251c\u2500\u2500 user_guide \u2502   \u2514\u2500\u2500 zzz_archive</p> <p>For documentation please go to https://effective-adventure-2kplkoq.pages.github.io/</p>"},{"location":"developer_guide/framework_setup/","title":"Framework Setup","text":""},{"location":"developer_guide/framework_setup/#fabric-workspace-setup","title":"Fabric Workspace Setup","text":"<p>Here we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below:</p> <ol> <li>Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace.</li> <li>Ensure that the workspace is Fabric enabled. If not, enable it.</li> <li>Make sure that there is at least one Datalake in the workspace.</li> <li>Get the connection details for the workspace. <ol> <li>Get the lakehouse name, the workspace id, and the lakehouse id. </li> <li>The lakehouse name and workspace name are easily viewed from the fabric / power bi portal.</li> <li>The easiest way to get this information is to <ol> <li>Navigate to a file or folder in the lakehouse, </li> <li>click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window. </li> <li>From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL. </li> <li>In the example below, the workspace id is <code>4f0cb887-047a-48a1-98c3-ebdb38c784c2</code> and the lakehouse id is <code>aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9</code>.</li> </ol> </li> </ol> </li> </ol> <p>https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks</p>"},{"location":"developer_guide/framework_setup/#create-dbt-project","title":"Create Dbt Project","text":"<p>Once you have taken note of the workspace id, lakehouse id, and lakehouse name, you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below:</p> <p>!&gt; Important Note when asked to select the adapter choose <code>dbt-fabricksparknb</code>. If you can't see the adapter, first install the dbt-fabricsparknb package from repository. During this process you will also be asked for the <code>workspace id</code>, <code>lakehouse id</code>, and <code>lakehouse name</code>. Use the values you gathered from the Power BI Portal. </p> <pre><code># Create your dbt project directories and profiles.yml file\ndbt init my_project # Note that the name of the project is arbitrary... call it whatever you like\n</code></pre> <p>The command above will create a new directory called <code>my_project</code>. Within this directory you will find a <code>dbt_project.yml</code> file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.:</p> <pre><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'my_project'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'my_project'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/\n# directory as views. These settings can be overridden in the individual model\n# files using the ` config(...) ` macro.\nmodels:\n  test4:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre> <p>The dbt init command will also update your <code>profiles.yml</code> file with a profile matching your dbt project name. Open this file in VS Code. This file can be found in \"./%USER_DIRECTORY%/.dbt/\"</p> <p>When run this will display a file similar to the one below. Check that your details are correct. </p> <pre><code>my_project:\n  outputs:\n    dev:\n      auth: cli #remove\n      client_id: dlkdjl #remove\n      client_scrent: dlkdjl #remove\n      connect_retries: 0 #remove\n      connect_timeout: 0 #remove\n      endpoint: dkld #remove\n      lakehouse: 'lakehouse' #the name of your lakehouse\n      lakehouseid: 'aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9' #the guid of your lakehouse\n      method: livy\n      schema: dbo #the schema you want to use\n      tenant_id: '72f988bf-86f1-41af-91ab-2d7cd011db47' #your power bi tenant id\n      threads: 1 #the number of threads to use\n      type: fabricsparknb #the type of adapter to use.. always use fabricsparknb\n      workspaceid: '4f0cb887-047a-48a1-98c3-ebdb38c784c2' #the guid of your workspace\n  target: dev\n</code></pre> <p>To complete the newly created project you will need to copy some directories from the project called \"ptfproj\" dbt project. Copy ptfproj/macros/ and ptfproj/metaextracts/ directories with their files into your new dbt project. Overwrite any directories or files if they exist. Now in metaextracts the file ListSchemas.json contains the lakehouses in your workspace. You can manually update this file.</p> <pre><code>[{\"namespace\":\"lh_raw\"},{\"namespace\":\"lh_conformed\"},{\"namespace\":\"lh_consolidated\"}]\n</code></pre>"},{"location":"developer_guide/framework_setup/#create-a-build-python-script","title":"Create a build python Script","text":"<p>This repo contains a dbt build python script test_pre_install.py in the root.</p> <pre><code>from dbt.adapters.fabricsparknb import utils as utils\nimport os \nimport sys\n\nutils.RunDbtProjectArg(PreInstall=True,argv = sys.argv)\n</code></pre> <p>You can execute this file by passing your project name as the parameter <pre><code>python test_pre_install.py my_project\n</code></pre></p> <p>If you get an error with Azure CLI connection issues or type errors. This is because the Profile.yaml file has the incorrect adaptor set. It should be \"fabricsparknb\" not \"fabricspark\".</p> <p>After successful execution and number of notebooks have been created in your project/target folder under notebooks. </p> <p>import_notebook.ipynb this will be used to import notebook files into your lakehouse.</p> <p>metadata_extract.ipynb is used to update the metadata json files in your project. </p> <p>The above two notebooks can be imported using the standard import notebooks function in fabric. The rest of the notebooks can be copied into your lakehouse Files/notebooks folder by running the following script in pwsh. </p> <pre><code>#Run upload.ps1\nInvoke-Expression -Command $env:DBT_PROJECT_DIR/target/pwsh/upload.ps1\n</code></pre> <p>You then open the import_notebook.ipynb in fabric and Run All to import the notebooks from the Files/Notebooks directory in fabric. </p> <p>Similar to  upload, using the following pwsh script will help you to download the metaextract files to the metaextrcats folder in repo.</p> <pre><code>#Run upload.ps1\nInvoke-Expression -Command $env:DBT_PROJECT_DIR/target/pwsh/download.ps1\n</code></pre> <p>Executing the master_notebook.ipynb notebook will execute all notebooks created in your project.</p> <p>This concludes the Framework setup.</p>"},{"location":"developer_guide/initial_setup/","title":"Initial setup","text":"<p>\ud83d\udce6dbt  \u2523 \ud83d\udcc2adapters  \u2503 \u2523 \ud83d\udcc2fabricsparknb  \u2503 \u2503 \u2523 \ud83d\udcdcinit.py  \u2503 \u2503 \u2523 \ud83d\udcdcversion.py  \u2503 \u2503 \u2523 \ud83d\udcdccatalog.py  \u2503 \u2503 \u2523 \ud83d\udcdcconnections.py  \u2503 \u2503 \u2523 \ud83d\udcdcfabric_spark_credentials.py  \u2503 \u2503 \u2523 \ud83d\udcdcimpl.py  \u2503 \u2503 \u2523 \ud83d\udcdclivysession.py  \u2503 \u2503 \u2523 \ud83d\udcdcmanifest.py  \u2503 \u2503 \u2523 \ud83d\udcdcmock.py  \u2503 \u2503 \u2523 \ud83d\udcdcnotebook.py  \u2503 \u2503 \u2517 \ud83d\udcdcutils.py  \u2503 \u2517 \ud83d\udcdcinit.py   \u2523 \ud83d\udcc2include  \u2503 \u2523 \ud83d\udcc2fabricsparknb  \u2503 \u2503 \u2523 \ud83d\udcc2macros  \u2503 \u2503 \u2503 \u2517 \ud83d\udcc2adapters  \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcschema.sql  \u2503 \u2503 \u2523 \ud83d\udcc2notebooks  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcimport_notebook.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmaster_notebook.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmaster_notebook_x.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmetadata_extract.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmodel_notebook.ipynb  \u2503 \u2503 \u2503 \u2517 \ud83d\udcdctest_notebook.ipynb  \u2503 \u2503 \u2523 \ud83d\udcc2pwsh  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcdownload.ps1  \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcupload.ps1  \u2503 \u2503 \u2523 \ud83d\udcdcinit.py  \u2503 \u2503 \u2523 \ud83d\udcdcdbt_project.yml  \u2503 \u2503 \u2523 \ud83d\udcdcmetadata_extract.ipynb  \u2503 \u2503 \u2517 \ud83d\udcdcprofile_template.yml  \u2503 \u2517 \ud83d\udcdcinit.py  \u2517 \ud83d\udcdcinit.py</p>"},{"location":"documentation_guide/","title":"Documentation Guide","text":""},{"location":"documentation_guide/#building-you-environment","title":"Building you environment","text":"<p>Documentation for this project is built using mkdocs-material. To contribute to the documentation you will need to create a separate python environment. I suggest that you call this <code>.env_mkdocs</code> to avoid confusion with the dbt environment. Create your environment and install the required packages as shown below:</p> <p>Important</p> <p>The commands below assume that you have already performed the <code>Core Tools Installation</code> steps in the User Guide. If you have not done this yet, please do so before proceeding. Note you ONLY have to install <code>core tools</code> it is not necessary to move on to the <code>other tools</code> section. </p> Create and activate the Python environment<pre><code># Create and activate the Python environment\npython -m venv .env_mkdocs\n.\\.env_mkdocs\\Scripts\\activate.ps1\npip install -r ./requirements_mkdocs.txt\n</code></pre>"},{"location":"documentation_guide/#updating-the-documentation","title":"Updating the documentation","text":"<p>The docucementation source is held in the <code>docs</code> directory. To update the documentation you will need to edit the markdown files in this directory. In order to understand the syntax used for the markdown be sure to review the reference section for mkdocs-material. Once you have made your changes you can build the documentation using the command below:</p> Build the documentation<pre><code>mkdocs build\n</code></pre> <p>To view the documentation locally you can use the command below:</p> View the documentation locally<pre><code>mkdocs serve\n</code></pre> <p>Tip</p> <p>The <code>mkdocs serve</code> command will start a local web server that will allow you to view the documentation in your browser. The server will also automatically rebuild the documentation when you make changes to the source files.</p> <p>Before publishing the documentation you should ensure that the documentation is up to date and that the changes are correct. You should also pull the latest from the repository to ensure that you are not overwriting someone else's changes. Do this by running the command below:</p> Pull the latest changes from the repository<pre><code>git pull\n</code></pre> <p>You can now publish the documentation to the repository by running the command below:</p> Publish the documentation<pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/dbt_build_process/","title":"Dbt Build Process","text":""},{"location":"user_guide/dbt_build_process/#run-the-build-script","title":"Run the build script","text":"<p>Run the build script that you created in the previous step using the code below in the terminal.</p> <p>Important</p> <p>Be sure to replace my_project with the name of your dbt project folder.</p> <pre><code>python post_install.py my_project \n</code></pre>"},{"location":"user_guide/dbt_build_process/#additional-steps-for-the-first-run","title":"Additional Steps for the First Run","text":"<p>The first time that you run this script you will get a warning that the metadata json files are not present. You will need to follow the steps in the warning message to download the metadata json files from your Fabric Lakehouse. These steps are as follows: </p> <ul> <li> In a pwsh terminal run the script below. This will set an environment variable that will be used by steps later on. Be sure to replace my_project with the name of your dbt project folder.     <pre><code>$env:DBT_PROJECT_DIR = \"my_project\"\n</code></pre></li> <li> In the same terminal now run the script below. This will upload a series of notebooks to a folder in your Fabric workspace     <pre><code>Invoke-Expression -Command $env:DBT_PROJECT_DIR/target/pwsh/upload.ps1\n</code></pre></li> <li> Now run the following code to output the path to the import notebook. This will be used to import the notebooks into your workspace:     <pre><code>Get-Childitem \"$env:DBT_PROJECT_DIR/target/notebooks/import_*\" | % {Write-Host $_.FullName}\n</code></pre></li> <li> Now login to the Fabric Portal and navigate to the workspace and lakehouse you are using and import the notebook using the path from the previous step.</li> </ul> <p>How to manually upload a notebook</p> <p>See https://learn.microsoft.com/en-us/fabric/data-engineering/how-to-use-notebook#import-existing-notebooks</p> <ul> <li> Open the notebook in the workspace and run all cells. This will upload the generated notebooks to your workspace.</li> <li> A new notebook should appear in the workspace called metadata_my_project_extract.ipynb where the text my_project is replaced with the name of your dbt_project. Open this notebook and run all cells. This will generate the metadata extract json files and place them in the metaextracts sub-directory of your lakehouse.</li> <li> Now, back in your powershell terminal. Run the script below. This will download the metadata extract json files to a subfolder in your dbt project directory called metaextracts.     <pre><code>Invoke-Expression -Command $env:DBT_PROJECT_DIR/target/pwsh/download.ps1\n</code></pre></li> </ul> <p>Important</p> <p>Note that the above script runs azcopy to download the metadata extract json files from the lakehouse. Prior to running the script you may need to log in to the appropriate environment. Usually this is done using <code>azcopy login</code>. However, if you have access to multiple Microsoft entra domains you may need to specify the tenant id. This can be done by running <code>azcopy login --tenant-id &lt;tenant_id&gt;</code>. </p> <ul> <li> Now re-run the dbt build script.     <pre><code>python post_install.py my_project \n</code></pre></li> </ul> <p>Important</p> <p>In the script above be sure to replace my_project with the name of your dbt project folder.</p> <p>When you run this build script successfully, you will see a series of notebooks generated in your my_project/target/notebooks directory. This is the <code>\"special sauce\"</code> of this dbt-adapter that allows your to run your dbt project natively as notebooks in a Fabric workspace. The image below shows a sample listing of generated notebooks. Your specific notebooks will be contain the name of your dbt project and may be different depending on the models and tests that you have defined in your dbt project. </p>"},{"location":"user_guide/dbt_build_process/#sample-listing-of-generated-notebooks","title":"Sample listing of Generated Notebooks","text":"<p>If you study the files shown above you will notice that there is a naming convention and that the notebooks are prefixed with a specific string. The following table explains at a high level the naming convention and the purpose of each type of notebook.</p> Notebook Prefix Description model. These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. test. These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. seed. These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. master_ These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality import_ This is a helper notebook that facilitate import of generated notebooks into workspace. metadata_ This is a helper notebook to facilitate generation of workspace metadata json files. <p>Important</p> <p>The green panels below provide a more detailed discussion of each type of notebook. Take a moment to expand each panel by clicking on it and read the detailed explanation of each type of notebook.</p> Notebooks with the Prefix <code>\"model.\"</code> <p>These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> <p></p> <p></p> Notebooks with the Prefix <code>\"test.\"</code> <p>These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> Notebooks with the Prefix <code>\"seed.\"</code> <p>These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> Notebooks with the Prefix <code>\"master_\"</code> <p>These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality.</p> Notebooks with the Prefix <code>\"import_\"</code> <p>This is a helper notebook that facilitates import of generated notebooks into workspace.</p> Notebooks with the Prefix <code>\"metadata_\"</code> <p>This is a helper notebook to facilitates the generation of workspace metadata json files.</p>"},{"location":"user_guide/dbt_build_process/#post-build-steps-checks","title":"Post Build Steps &amp; Checks","text":"<p>After a successful execution you can now upload your notebooks to your Fabric Workspace and run your dbt transformations against your lakehouse/s. To do this follow the steps below: </p> <ul> <li> In a pwsh terminal run the script below. This will set an environment variable that will be used by steps later on. Be sure to replace my_project with the name of your dbt project folder.     <pre><code>$env:DBT_PROJECT_DIR = \"my_project\"\n</code></pre></li> <li> In the same terminal now run the script below. This will upload a the generated notebooks to a folder in your Fabric workspace     <pre><code>Invoke-Expression -Command $env:DBT_PROJECT_DIR/target/pwsh/upload.ps1\n</code></pre></li> <li> In your Fabric workspace open the import_my_project_notebook and run all cells. This will upload the generated notebooks to your workspace. If they already exist they will be updated. In order to organise your workspace efficiently I suggest that you move them all into a folder called the matches the name of your dbt project. Don't worry, even if you upload a new version of the notebooks the folder structure will be mantained.</li> </ul>"},{"location":"user_guide/dbt_project_setup/","title":"DBT Project Setup","text":""},{"location":"user_guide/dbt_project_setup/#fabric-workspace-setup","title":"Fabric Workspace Setup","text":"<p>Next we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below:</p> <ol> <li>Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace.</li> <li>Ensure that the workspace is Fabric enabled. If not, enable it.</li> <li>Make sure that there is at least one Datalake in the workspace.</li> <li> <p>Get the connection details for the workspace.</p> <ol> <li>You will need to get the workspace name, workspace id, lakehouse id, and lakehouse name. </li> <li>The lakehouse name and workspace name are easily viewed from the fabric / power bi portal. </li> <li>The easiest way to get the id information is to:<ol> <li>Navigate to a file or folder in your target lakehouse.</li> <li>Click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window.</li> <li>From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL.</li> <li>In the example below, the workspace id is <code>4f0cb887-047a-48a1-98c3-ebdb38c784c2</code> and the lakehouse id is <code>aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9</code>.</li> </ol> </li> </ol> </li> </ol> Example URL<pre><code>https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks\n</code></pre>"},{"location":"user_guide/dbt_project_setup/#create-dbt-project","title":"Create Dbt Project","text":"<p>Once you have taken note of the workspace id, lakehouse id, workspace name and lakehouse name you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below:</p> <pre><code># Create your dbt project directories and profiles.yml file\ndbt init my_project # Note that the name of the project is arbitrary... call it whatever you like\n</code></pre> <p>When asked the questions below, provide the answers in bold below:</p> <ol> <li><code>Which data base would you like to use?</code> select <code>dbt-fabricksparknb</code></li> <li><code>Desired authentication method option (enter a number):</code> select <code>livy</code></li> <li><code>workspaceid (GUID of the workspace. Open the workspace from fabric.microsoft.com and copy the workspace url):</code> Enter the workspace id</li> <li><code>lakehouse (Name of the Lakehouse in the workspace that you want to connect to):</code> Enter the lakehouse name</li> <li><code>lakehouseid (GUID of the lakehouse, which can be extracted from url when you open lakehouse artifact from fabric.microsoft.com):</code> Enter the lakehouse id</li> <li><code>endpoint [https://api.fabric.microsoft.com/v1]:</code> Press enter to accept the default</li> <li><code>auth (Use CLI (az login) for interactive execution or SPN for automation) [CLI]:</code> select <code>cli</code></li> <li><code>client_id (Use when SPN auth is used.):</code> Enter a single space and press enter</li> <li><code>client_scrent (Use when SPN auth is used.):</code> Enter a single space and press enter</li> <li><code>tenant_id (Use when SPN auth is used.):</code> Enter a single space or Enter your PowerBI tenant id</li> <li><code>connect_retries [0]:</code> Enter 0</li> <li><code>connect_timeout [10]:</code> Enter 10</li> <li><code>schema (default schema that dbt will build objects in):</code> Enter <code>dbo</code></li> <li>threads (1 or more) [1]: Enter 1</li> </ol> <p>The command above will create a new directory called <code>my_project</code>. Within this directory you will find a <code>dbt_project.yml</code> file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.:</p> dbt_project.yml<pre><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'my_project'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'my_project'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\nmodels:\n  test4:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre> <p>The dbt init command will also update your <code>profiles.yml</code> file with a profile matching your dbt project name. Open this file in your favourite text editor using the command below:</p> WindowsMacOSLinux <pre><code>code  $home/.dbt/profiles.yml\n</code></pre> <pre><code>code  ~/.dbt/profiles.yml\n</code></pre> <pre><code>code  ~/.dbt/profiles.yml\n</code></pre> <p>When run this will display a file similar to the one below. Check that your details are correct.</p> <p>Note</p> <p>The <code>profiles.yml</code> file should look like the example below except that in your case the highlighted lines may contain different values.</p> profiles.yml<pre><code>my_project:\n  target: my_project_target\n  outputs:\n    my_project_target:\n      authentication: CLI\n      method: livy\n      connect_retries: 0\n      connect_timeout: 10\n      endpoint: https://api.fabric.microsoft.com/v1\n      workspaceid: 4f0cb887-047a-48a1-98c3-ebdb38c784c2\n      workspacename: test\n      lakehousedatapath: /lakehouse\n      lakehouseid: 031feff6-071d-42df-818a-984771c083c4\n      lakehouse: datalake\n      schema: dbo\n      threads: 1\n      type: fabricsparknb\n      retry_all: true\n</code></pre> <p>Now we are ready to run our dbt project for the first time. But first we need to create a build script.</p>"},{"location":"user_guide/dbt_project_setup/#create-a-python-build-script-that-will-wrap-our-dbt-process","title":"Create a python build script that will wrap our dbt process","text":"<p>This repository contains a dbt build script created in python. Make a copy of this script by copying the code found at https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb/blob/main/test_post_install.py. Alternatively, you can copy the code in the code block titled Python Build script template below. Paste the code into a new file in the root of your source code directory. You can create this file using the vscode command line shown in the code block titled New file creation in vscode below.</p> <p>Important</p> <p>Be sure to change the line <code>os.environ['DBT_PROJECT_DIR'] = \"testproj\"</code> by replacing \"testproj\" with the folder name of your dbt project.</p> New file creation in vscode<pre><code>code post_install.py\n</code></pre> Python Build script template<pre><code>from dbt.adapters.fabricsparknb import utils as utils\nimport os\nimport sys\n\nutils.RunDbtProjectArg(PreInstall=False,argv = sys.argv)\n</code></pre> <p>Info</p> <p>You are now ready to move to the next step in which you will build your dbt project. Follow the Dbt Build Process guide.</p>"},{"location":"user_guide/development_workflow/","title":"Development Workflow","text":""},{"location":"user_guide/development_workflow/#development-and-deployment-flow-using-fabric-spark-notebook-adapter","title":"Development and Deployment Flow Using Fabric Spark Notebook Adapter","text":"<p>Advantages</p> <ul> <li> Available today</li> <li> Native Fabric Notebooks Generated and Deployed<ol> <li>Non dbt users able to view notebooks and business logic</li> <li>Monitoring and debugging of loads directly in Fabric without the need for a separate tool</li> </ol> </li> <li> Re-occurring loads achieved using native Fabric scheduling </li> <li> Simplified code promotion process using native Fabric Git integration</li> <li> No need for dbt hosted in a virtual machine <ol> <li>No need for service account</li> <li>No need for Azure Landing Zone</li> <li>No need for secure network connectivity between Azure VM and Fabric   </li> </ol> </li> <li> Allows for disconnected development environment providing<ol> <li>Faster DBT build times</li> <li>Greater developer flexibility</li> </ol> </li> <li> Simplified code promotion Process using native Fabric Git integration<ol> <li>Single, native promotion process for all Fabric artifacts including non-dbt ones</li> </ol> </li> </ul> <p>Disadvantages</p> <ul> <li>Requires Additional Steps<ol> <li>Meta Data Extract</li> <li>Notebook Upload </li> <li>Notebook Import</li> </ol> </li> </ul>"},{"location":"user_guide/development_workflow/#development-and-deployment-flow-using-original-fabric-spark-adapter","title":"Development and Deployment Flow Using Original Fabric Spark Adapter","text":""},{"location":"user_guide/development_workflow/#detailed-workflow","title":"Detailed Workflow","text":"<p>Inital Setup 1. Provision Workspace    - Development Environment: Fabric Portal    - Re-occurence: Do once per development environment set-up    - Instructions: Create a new workspace in the Power BI Portal, or use an existing workspace.</p> <ol> <li>Get Workspace Connection Details</li> <li>Development Environment: Fabric Portal</li> <li>Re-occurence: Do once per development environment set-up</li> <li> <p>Instructions: Get the workspace connection details from the Power BI Portal.</p> </li> <li> <p>Create or Update <code>profiles.yml</code></p> </li> <li>Development Environment: VS Code on local, developemnt machine</li> <li></li> <li> <p>Create or Update <code>dbt_project.yml</code> </p> </li> <li>Build Project</li> <li>Manually Upload Notebooks </li> <li>Run Meta Data Extract</li> </ol> <p>Ongoing Development Cycle</p> <ol> <li> <p>Download Metadata: </p> </li> <li> <p>Update Dbt Project </p> </li> <li>Build Dbt Project </li> <li>Verify Outputs </li> <li>Update Notebooks     <ol> <li>Upload to Onelake</li> <li>Update to GIT repo</li> </ol> </li> <li>Promote to Workspace     <ol> <li>Run Import Notebook</li> <li>Promote GIT branch</li> </ol> </li> <li>Run Master Notebook </li> <li>Validate Results </li> <li>Run Metadata Extract</li> </ol>"},{"location":"user_guide/fabric_ci_cd_process/","title":"Fabric CI/CD with Git Deployment","text":""},{"location":"user_guide/fabric_ci_cd_process/#git-based-deployment","title":"Git Based Deployment","text":"<p>The initial setup is based on a Git branch that is linked to all workspaces. As illustrated in the given example, we have described three stages: Development, Test, and Production. It also employs feature branches for individual developments within isolated workspaces using branch out functionality.</p> <p>The successful operation of this scenario depends on branching, merging, and pull requests.</p> <ol> <li><code>Each workspace is assigned its own branch.</code></li> <li><code>The introduction of new features is facilitated by raising pull requests.</code></li> <li><code>All deployments are initiated from the repository.</code></li> <li><code>To transition from Development to Test, and subsequently from Test to Production, a pull request must be initiated from the originating stage.</code></li> </ol> <p>The synchronization between the Git branch and the workspace can be automated. This is achieved by invoking the Git Sync API as part of a build pipelines, which is automatically triggered following the approval of a pull request.</p> <p></p>"},{"location":"user_guide/fabric_ci_cd_process/#git-and-build-environment-based-deployment","title":"Git and Build Environment Based Deployment","text":"<p>Git is exclusively linked to the Development workspace. The deployment to other stages is executed based on Build environments. This implies that the Fabric Item APIs are utilized to perform Create, Read, Update or Delete operations.</p> <p>Key points of this setup are:</p> <ol> <li><code>The Git repository serves as the foundation for creating, updating, or deleting items in the workspace.</code></li> <li><code>Git is solely connected to the Development workspace.</code></li> <li><code>Following a pull request, a Build pipeline is activated.</code></li> <li><code>The Build pipeline executes operations to the workspace.</code></li> </ol> <p>Note</p> <pre><code>This approach is code-intensive and for each future item to be supported, modifications may be required in the Build pipelines.\n</code></pre> <p></p>"},{"location":"user_guide/fabric_ci_cd_process/#git-and-fabric-deployment-pipeline-based-deployment","title":"Git and Fabric Deployment Pipeline Based Deployment","text":"<p>This is based on Fabric Deployment pipelines. This user-friendly interface simplifies the deployment process from one stage to another and is less code-intensive.</p> <p>Git is solely connected to the Development workspace, and feature branches continue to exist in separate workspaces. However, the Test, Production, and any additional workspaces are not linked to Git.</p> <p>Key aspects of this setup include:</p> <ol> <li><code>The release process to other stages, such as Test and Production, is managed via Deployment Pipelines in the Fabric.</code></li> <li><code>The Development workspace is the only one connected to Git.</code></li> <li><code>Triggers for the Fabric deployment pipeline can be automated. This is achieved by using Build Pipelines, which are automatically activated following the approval of a pull request.</code></li> </ol> <p>These pipelines can call the Fabric REST API and can also be integrated with Git Sync API for synchronizing the development workspace.</p> <p></p>"},{"location":"user_guide/initial_setup/","title":"Environment Setup","text":"<p>This section outlines the steps required to setup the development environment to use this dbt-adapter as part of a dbt data transformation project.</p> <p>To provide a common, cross-platform set of instructions we will first install Powershell. To facilitate the installation process we will use package managers such as winget for Windows, brew for MacOS and <code>apt</code> for Linux.</p>"},{"location":"user_guide/initial_setup/#core-tools-installation","title":"Core Tools Installation","text":"WindowsMacOSLinux <pre><code># Winget Installs \nwinget install Microsoft.PowerShell\n</code></pre> <pre><code>brew install powershell/tap/powershell\n</code></pre> <pre><code># TBA\n</code></pre> <p>Next we will install Python and development tools such as vscode.</p> WindowsMacOSLinux <pre><code># Winget Installs \nwinget install -e --id Python.Python -v 3.12\nwinget install -e --id Microsoft.VisualStudioCode\nwinget install --id Git.Git -e --source winget\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n</code></pre> <pre><code># Brew Installs\nbrew install python@3.12\nbrew install --cask visual-studio-code\nbrew install git\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n\n# TODO \n# Add OSX AZ Copy Instructions\n</code></pre> <pre><code># TBA\n</code></pre>"},{"location":"user_guide/initial_setup/#other-tools","title":"Other tools","text":"<p>Now that we have pwsh installed, we can use it as a cross platform shell to install the additional required tools. </p> WindowsMacOSLinux <pre><code># Az Copy Install - No Winget Package Available\nInvoke-WebRequest -Uri https://aka.ms/downloadazcopy-v10-windows -OutFile AzCopy.zip -UseBasicParsing\nExpand-Archive ./AzCopy.zip ./AzCopy -Force\nNew-Item -ItemType \"directory\" -Path \"$home/AzCopy\"  -Force  \nGet-ChildItem ./AzCopy/*/azcopy.exe | Move-Item -Destination \"$home\\AzCopy\\AzCopy.exe\" -Force  \n$userenv = [System.Environment]::GetEnvironmentVariable(\"Path\", \"User\") \n[System.Environment]::SetEnvironmentVariable(\"PATH\", $userenv + \";$home\\AzCopy\", \"User\")\nRemove-Item .\\AzCopy\\ -Force\nRemove-Item AzCopy.zip -Force\n</code></pre> <pre><code># TODO \n# Add OSX AZ Copy Instructions\n</code></pre> <pre><code># TBA\n</code></pre>"},{"location":"user_guide/initial_setup/#source-directory-python-env","title":"Source Directory &amp; Python Env","text":"<p>Now lets create and activate our Python environment and install the required packages.</p> <p>Tip</p> <p>When doing pip install dbt-fabricspark below it can take a few minutes to complete on some machines. Occasionally pip may get stuck and in such cases break the execution using ctrl-c and run the same pip again. </p> WindowsMacOSLinux <pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create and activate the Python environment\npython -m venv .env\n./.env/Scripts/Activate.ps1\n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre> <pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create and activate the Python environment\npython -m venv .env\n./.env/Scripts/Activate.ps1  \n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre> <pre><code># TBA\n</code></pre> <p>Info</p> <p>You are now ready to move to the next step in which you will set up your dbt project. Follow the Dbt Project Setup guide.</p>"}]}